{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator\n",
    "According to the paper, the discriminator has the following architecture.  \n",
    "![Discriminator](Images/Discriminator.jpg)  \n",
    "I designed the above picture using [alexlenail.me](http://alexlenail.me/NN-SVG/AlexNet.html).  \n",
    "This architecture, however, has minor differences compared to the original diagram shown in the article. To begin with, it is apparent that instead of a 5x5 filter, here is 4x4. The reason is because of the following formula:\n",
    "$$n_{out} = \\lfloor\\frac{n_{in} + 2p - k}{s}\\rfloor + 1$$\n",
    "$where$  \n",
    "$n_{in}$ : **number of input features**  \n",
    "$n_{out}$: **number of output features**  \n",
    "$k$: **convolution kernel size**  \n",
    "$p$: **convolution padding size**  \n",
    "$s$: **convolution stride size**  \n",
    "As we see from the formula, if $n_{in} = 64$, $p = 1$, $k = 4$ and $s = 1$, then $n_{out} = 32$; however, the calculation is not right if we set $k = 5$ without padding.\n",
    "\n",
    "The order is also inverted from the original architecture, which is a noteworthy change. This is due to the fact that the paper's illustrated architecture was intended for the generator. Last but not least, we have 1-dimensional output at the result of this network since we need to evaluate whether the provided input picture is fake or not.\n",
    "\n",
    "There are other points that are important to be mentioned which cannot be inferred from the picture above:\n",
    "* Batchnorm is used for all layers\n",
    "* LeakyReLU activation is used for all layers with 0.2 as the slope of the leak\n",
    "* Since applying batchnorm to all layers cause sample oscillation and model instability, the paper suggested to avoid using batchnorm in the discriminator input layer.\n",
    "* the last convolution layer is flattened and then fed into a single sigmoid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_depth, feature_depth):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.leaky_relu_negative_slope = 0.2\n",
    "        self.discriminator = nn.Sequential(\n",
    "            self.input_conv_block(in_channels=input_depth, out_channels=feature_depth),  #  3x64x64 -> 128x32x32\n",
    "            self.conv_block(in_channels=feature_depth, out_channels=feature_depth * 2),   #  128x32x32 -> 256x16x16\n",
    "            self.conv_block(in_channels=feature_depth * 2, out_channels=feature_depth * 4),   #  256x16x16 -> 512x8x8\n",
    "            self.conv_block(in_channels=feature_depth * 4, out_channels=feature_depth * 8),   #  512x8x8 -> 1024x4x4\n",
    "            self.conv_block(in_channels=feature_depth * 8, out_channels=1, padding=0),   #  1024x4x4 -> 1x1x1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def input_conv_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=4,\n",
    "                      stride=2,\n",
    "                      padding=1),\n",
    "            nn.LeakyReLU(self.leaky_relu_negative_slope)\n",
    "        )\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size,\n",
    "                      stride,\n",
    "                      padding,\n",
    "                      bias=False,  # Since we use batchnorm, it is not necessary to use bias\n",
    "                     ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_negative_slope)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
